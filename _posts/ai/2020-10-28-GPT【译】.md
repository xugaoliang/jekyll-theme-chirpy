---
title: "GPT【译】"
categories:
    - AI
tags:
  - NLP
---

论文：Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

自然语言理解包括多种多样的任务，例如文本范围，问题解答，语义相似性评估和文档分类。尽管大型的未标记文本语料库很丰富，但是用于学习这些特定任务的标记数据却很少，这使得经过严格训练的模型难以充分发挥作用。我们证明，通过在各种未标记文本的语料库上对语言模型进行生成式预训练，然后对每个特定任务进行区分性微调，可以实现这些任务的巨大收益。与以前的方法相比，我们在微调过程中利用了任务感知的输入转换来实现有效的传递，同时对模型体系结构的更改要求最小。我们在广泛的自然语言理解基准测试中证明了我们的方法的有效性。我们的一般任务不可知模型优于使用专门为每个任务设计的体系的经过严格训练的模型，在研究的12个任务中有9个显着改善了现有技术水平。例如，我们在常识推理（Stories Cloze测验）上实现了8.9％的绝对改进，在回答问题（RACE）上实现了5.7％的绝对改进，在文本蕴涵方面（MultiNLI）实现了1.5％的绝对改进


从原始文本有效学习的能力对于减轻自然语言处理（NLP）中对监督学习的依赖性至关重要。大多数深度学习方法需要大量的手动标记数据，这限制了它们在许多领域中的应用，这些领域饱受注释资源的困扰[61]。在这些情况下，可以利用来自未标记数据的语言信息的模型为收集更多注释提供了宝贵的替代方法，这可能既耗时又昂贵。此外，即使在可以进行大量监督的情况下，以无监督的方式学习良好的表示形式也可以显着提高性能。到目前为止，最有说服力的证据是广泛使用预训练的单词嵌入[10、39、42]来提高一系列NLP任务的性能[8、11、26、45]。但是，利用未标记文本中的单词级信息以外的信息具有挑战性，主要有两个原因。首先，尚不清楚哪种类型的优化目标最有效地学习对传输有用的文本表示形式。最近的研究已经关注了各种目标，例如语言建模[44]，机器翻译[38]和语篇连贯[22]，每种方法在不同任务上的表现均优于其他方法。1其次，对于实现这一目标的最有效方法尚无共识将这些学习的表示形式转移到目标任务。现有技术涉及使用复杂的学习方案[21]和添加辅助学习目标[50]对模型体系结构进行特定于任务的更改[43、44]的组合。这些不确定性使得难以开发有效的半监督学习方法进行语言处理

在本文中，我们探索了一种无监督的方法，该方法结合了无监督的预训练和有监督的微调功能来进行语言理解任务。我们的目标是学习一种通用的表示形式，该表示形式几乎无法适应各种任务。我们假设可以访问大量未标记文本和带有手动注释的训练示例（目标任务）的多个数据集。我们的设置不需要这些目标任务与未标记的语料库位于同一域中。我们采用两阶段的培训程序。首先，我们对未标记的数据使用语言建模目标，以学习神经网络模型的初始参数。随后，我们使用相应的监督目标将这些参数调整为目标任务。对于我们的模型架构，我们使用了Transformer [62]，它已被证明在各种任务上表现出色，例如机器翻译[62]，文档生成[34]和语法解析[29]。与诸如递归网络之类的替代方案相比，此模型选择为我们提供了更结构化的内存来处理文本中的长期依赖性，从而在各种任务之间实现了强大的传输性能。在传输过程中，我们利用从遍历样式方法[52]导出的特定于任务的输入改编，该方法将结构化文本输入作为单个连续的令牌序列进行处理。正如我们在实验中演示的那样，这些调整使我们能够以最小的方式对预训练模型的架构进行有效调整。我们在四种类型的语言理解任务上评估我们的方法-自然语言推理，问题回答，语义相似性和文本分类。我们的通用任务不可知模型优于采用专门为每个任务设计的体系的经过严格训练的模型，在研究的12个任务中有9个显着改善了现有技术水平。例如，我们在常识推理（Stories Cloze测验）[40]上实现了8.9％的绝对改进，在问答（RACE）[30]上实现了5.7％的改进，在文本蕴涵（MultiNLI）[66]方面的改进了5.5％最近推出了GLUE多任务基准测试[64]。我们还分析了预训练模型在四个不同设置下的零射行为，并证明它为下游任务获得了有用的语言知识

NLP的半监督学习我们的工作大致属于自然语言的半监督学习。随着应用到诸如序列标记[24、33、57]或文本分类[41、70]之类的任务，这种范例引起了极大的兴趣。最早的方法是使用未标记的数据来计算单词级别或短语级别的统计信息，然后将其用作监督模型中的特征[33]。在过去的几年中，研究人员已经证明了使用词嵌入[11，39，42]的好处，这些词嵌入在未标记的语料库上进行了训练，可以提高在各种任务上的表现[8，11，26，45]。但是，这些方法主要传输单词级别的信息，而我们的目标是捕获更高级别的语义。最近的方法已经研究了从未标记的数据中学习和利用超过单词级语义的方法。可以使用未标记的语料库训练的短语级或句子级嵌入，已将文本编码为适用于各种目标任务的适当矢量表示形式[28、32、1、36、22、12、56、31]

无监督的预训练无监督的预训练是半监督学习的一种特殊情况，其目标是找到一个好的初始化点，而不是修改有监督的学习目标。早期的工作探索了该技术在图像分类[20，49，63]和回归任务[3]中的使用。随后的研究[15]证明了预训练是一种正则化方案，可以在深度神经网络中更好地进行泛化。在最近的工作中，该方法已用于帮助在各种任务上训练深度神经网络，例如图像分类[69]，语音识别[68]，实体歧义消除[17]和机器翻译[48]。与我们最接近的工作是使用语言建模目标对神经网络进行预训练，然后在监督下对目标任务进行微调。戴等。 [13]和Howard and Ruder [21]遵循这种方法来改善文本分类。但是，尽管预训练阶段有助于捕获某些语言信息，但是LSTM模型的使用限制了它们的预测能力。相反，如我们的实验所示，我们选择的变压器网络使我们能够捕获更长距离的语言结构。此外，我们还证明了我们的模型在更广泛的任务（包括自然语言推断，释义检测和故事完成）上的有效性。其他方法[43、44、38]在训练目标任务的监督模型时，将来自预训练语言或机器翻译模型的隐藏表示用作辅助功能。这涉及到每个单独的目标任务的大量新参数，而在传输过程中，我们需要对模型体系结构进行最小的更改。辅助培训目标添加辅助无监督培训目标是半监督学习的另一种形式。 Collobert和Weston [10]的早期工作使用了各种各样的辅助NLP任务，例如POS标记，分块，命名实体识别和语言建模，以改善语义角色标记。最近，Rei [50]在目标任务目标中添加了辅助语言建模目标，并证明了序列标记任务的性能提升。我们的实验还使用了一个辅助目标，但是正如我们所展示的，无监督的预训练已经学习了与目标任务相关的几个语言方面

我们的培训过程包括两个阶段。 第一步是在大量文本语料库上学习大容量语言模型。 接下来是微调阶段，在此阶段，我们将模型修改为带有标记数据的判别任务

给定令牌的无监督语料U = {u1，。 。 。 ，un}，我们使用标准的语言建模目标来最大化以下可能性

其中k是上下文窗口的大小，条件概率P使用参数为Θ的神经网络建模。 这些参数是使用随机梯度下降法训练的[51]。 在我们的实验中，我们将多层Transformer解码器[34]用于语言模型，这是变压器[62]的一种变体。 该模型对输入上下文令牌应用多头自我注意操作，然后对位置前馈层进行分层，以在目标令牌上产生输出分布

其中U =（uk，...，u-1）是令牌的上下文向量，n是层数，Weis令牌嵌入矩阵，Wpis位置嵌入矩阵

用等式中的目标训练模型后。 1，我们将参数调整为受监督的目标任务。 我们假设一个标记数据集C，其中每个实例都由一系列输入标记x1组成。 。 。 ，xm和标签y。 输入经过我们的预训练模型，以获得最终变压器块的激活hm l，然后将其输入到具有参数Wy的附加线性输出层中以预测y

这给了我们以下最大化目标

我们还发现，将语言建模作为微调的辅助目标，可以通过（a）改进监督模型的泛化以及（b）加速收敛来帮助学习。 这与先前的工作[50，43]一致，后者也观察到使用这种辅助物镜可以改善性能。 具体来说，我们优化以下目标（权重为λ

总的来说，在微调期间我们唯一需要的额外参数是Wy，以及定界符标记的嵌入（在下面的第3.3节中进行介绍）

对于某些任务，例如文本分类，我们可以如上所述直接调整模型。某些其他任务（例如问题回答或文本要求）具有结构化的输入，例如有序句子对或文档，问题和答案的三元组。由于我们的预训练模型是在连续的文本序列上训练的，因此我们需要进行一些修改才能将其应用于这些任务。先前的工作提出了基于转移表示的学习任务特定体系结构[44]。这种方法重新引入了大量特定于任务的自定义，并且不对这些其他体系结构组件使用转移学习。取而代之的是，我们使用遍历风格的方法[52]，其中将结构化输入转换为有序序列，我们的预训练模型可以处理该序列。这些输入转换使我们可以避免跨任务对体系结构进行大量更改。我们在下面提供了这些输入转换的简要说明，图1提供了直观的图示。所有转换都包括添加随机初始化的开始和结束标记（？s？，？e

文本蕴涵对于蕴涵任务，我们将前提p和假设h令牌序列连接在一起，并在两者之间使用定界符（$）

相似度对于相似度任务，要比较的两个句子没有固有的顺序。 为了反映这一点，我们修改了输入序列以包含两个可能的句子顺序（之间有一个定界符），并分别独立处理以产生两个序列表示hm l，它们在被馈送到线性输出层之前逐个元素地添加。 问题解答和常识推理对于这些任务，我们获得了一个上下文文档z，一个问题q和一组可能的答案{ak}。 我们将文档上下文和问题与每个可能的答案连接起来，在两者之间添加定界符标记以获得[z; q; $; ak]。 这些序列中的每一个都由我们的模型独立处理，然后通过softmax层进行归一化以产生可能的答案的输出分布

无监督的预训练我们使用BooksCorpus数据集[71]来训练语言模型。 它包含7,000多种不同类型的未出版的未出版书籍，包括冒险，幻想和浪漫。 至关重要的是，它包含长段连续的文本，这使生成模型可以学习以远程信息为条件。 另一种数据集1B Word Benchmark被类似的方法ELMo [44]使用，其大小大致相同但在句子级别进行了洗牌-破坏了远程结构。 我们的语言模型在该语料库上实现了极低的令牌级别困惑度18.4

模型规格我们的模型主要遵循原始的变压器工作[62]。我们训练了一个12层仅解码器的变压器，该变压器具有带遮罩的自注意力头（768维状态和12个注意力头）。对于位置前馈网络，我们使用了3072维内部状态。我们使用亚当优化方案[27]，最大学习率为2.5e-4。在最初的2000次更新中，学习率从零开始线性增加，并使用余弦时间表将其退火为零。我们在512个令牌的64个随机采样，连续序列的小批量上训练100个纪元。由于layernorm [2]在整个模型中被广泛使用，因此简单的N（0，0.02）权重初始化就足够了。我们使用字节对编码（BPE）词汇表进行了40,000次合并[53]以及残差，嵌入和注意遗漏，且速率为0.1，用于正则化。我们还采用了[37]中提出的L2正则化的修改版本，所有非偏置或增益权重均w = 0.01。对于激活函数，我们使用了高斯误差线性单位（GELU）[18]。我们使用学习的位置嵌入来代替原始工作中提出的正弦形式。我们使用ftfy库2来清理BooksCorpus中的原始文本，标准化一些标点符号和空格，并使用spaCy标记器。3细调细节除非指定，否则我们将重用无监督预训练中的超参数设置。我们以0.1的比率向分类器添加辍学率。对于大多数任务，我们使用的学习率为6.25e-5，批处理大小为32。我们的模型可以快速进行微调，并且对于大多数情况而言，经过3个时期的训练就足够了。我们使用线性学习率衰减时间表，其中热身超过训练的0.2％。 λ设为0.5

我们对各种有监督的任务进行实验，包括自然语言推理，问题解答，语义相似性和文本分类。其中一些任务可作为我们最近使用的GLUE多任务基准测试[64]的一部分获得。图1概述了所有任务和数据集。自然语言推论自然语言推论（NLI）的任务，也称为识别文本蕴涵，涉及读取一对句子，并根据蕴涵，矛盾或中立来判断它们之间的关系。尽管最近有很多兴趣[58、35、44]，但是由于存在多种现象，例如词汇蕴含，共指以及词汇和句法歧义，该任务仍然具有挑战性。我们评估了五个具有不同来源的数据集，包括图像标题（SNLI），转录语音，流行小说和政府报告（MNLI），维基百科文章（QNLI），科学考试（SciTail）或新闻文章（RTE）。表2详细列出了针对我们的模型和以前的最新方法的不同NLI任务的各种结果。我们的方法在五个数据集中的四个数据集上均明显优于基线，与之前的最佳结果相比，MNLI的绝对改进高达1.5％，SciTail的改进为5％，QNLI的改进为5.8％，SNLI的改进为0.6％。这表明我们的模型能够更好地对多个句子进行推理，并能处理语言歧义性。在RTE（我们评估的较小数据集之一）（2490个示例）上，我们达到了56％的准确度，低于多任务biLSTM模型报告的61.7％。鉴于我们的方法在较大的NLI数据集上的强大性能，我们的模型也可能会从多任务训练中受益，但是我们目前尚未对此进行探索

问题解答和常识推理问题解答是另一项需要单句和多句推理的方面的任务。我们使用最近发布的RACE数据集[30]，该数据集包含英语段落以及中学和高中考试的相关问题。与其他数据集（如CNN [19]或SQuaD [47]）相比，该语料库还包含更多推理类型的问题，为我们的模型提供了完美的评估，该模型经过训练可以处理远程上下文。另外，我们根据故事完结测试[40]进行评估，该测试涉及从两个选项中选择多句故事的正确结尾。在这些任务上，我们的模型再次以明显的优势胜过以前的最佳结果-Story Cloze高达8.9％，RACE总体为5.7％。这证明了我们的模型有效地处理远程上下文的能力。语义相似度语义相似度（或释义检测）任务涉及预测两个句子在语义上是否相等。挑战在于认识概念的改写，理解否定以及处理句法歧义。我们为此任务使用了三个数据集– Microsoft释义语料库（MRPC）[14]（从新闻来源收集），Quora问题对（QQP）数据集[9]和语义文本相似性基准（STS-B）[6]。 ]。我们获得了三个语义相似性任务中的两个（表4）的最新结果，并且在STS-B上获得了1分的绝对增益。 QQP的性能差异很大，与单任务BiLSTM + ELMo + Attn相比，绝对提高了4.2％。分类最后，我们还将评估两个不同的文本分类任务。语言可接受性语料库（CoLA）[65]包含关于句子是否符合语法的专家判断，并测试经过训练的模型的先天语言偏向。另一方面，斯坦福情感树库（SST-2）[54]是标准的二进制分类任务。我们的模型在CoLA上获得了45.4分，这比之前的最佳结果35.0分特别高，显示了我们模型所固有的语言偏见。该模型在SST-2上还达到了91.3％的准确度，与最新结果相媲美。在GLUE基准测试中，我们还获得了72.8的总体得分，明显优于之前的68.9的最高得分

总体而言，我们的方法在我们评估的12个数据集中的9个数据集中有9个获得了最新的最新结果，在许多情况下均表现出优异的整体效果。 我们的结果还表明，我们的方法适用于不同大小的数据集，从较小的数据集，例如STS-B（≈5.7k训练示例）到最大的SNLI（≈550k训练示例）

转移的层数的影响我们观察了将可变数目的层数从无监督的预训练转移到有监督的目标任务的影响。 图2（左）说明了我们在MultiNLI和RACE上的方法的性能与传输层数的关系。 我们观察到了标准结果，即转移嵌入物可改善性能，并且每个变压器层可为MultiNLI上的完全转移提供高达9％的进一步收益。 这表明预训练模型中的每一层都包含解决目标任务的有用功能

零镜头行为我们想更好地理解为什么对变压器进行语言模型预训练是有效的。 一个假设是，潜在的生成模型学习执行我们评估的许多任务，以提高其语言建模能力，并且更加结构化

