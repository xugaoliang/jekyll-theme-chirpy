---
title: "GPT3【译】"
categories:
    - AI
tags:
  - NLP
---

论文：Tom B. Brown, Benjamin Mann, Nick Ryder, David Luan, Melanie Subbiah 等人. [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)

通过对大量文本进行预训练，然后对特定任务进行微调，最近的工作证明了在许多NLP任务和基准方面的巨大收获。尽管在结构上通常与任务无关，但是此方法仍然需要特定于任务的微调数据集，该数据集包含成千上万个示例。相比之下，人类通常只能通过几个示例或简单的指令来执行新的语言任务-当前的NLP系统在很大程度上仍难以做到这一点。在这里，我们证明了扩展语言模型可以极大地提高与任务无关的性能，而且很少出现问题，有时甚至可以通过现有的最新微调方法达到竞争力。具体来说，我们训练了GPT-3（一种具有1750亿个参数的自回归语言模型），比以前的任何非稀疏语言模型都多10倍，并在少数测试条件下测试其性能。对于所有任务，应用GPT-3时不会进行任何渐变更新或微调，而仅通过与模型的文本交互来指定任务和少量演示。 GPT-3在许多NLP数据集上均具有出色的性能，包括翻译，问题解答和完形填空任务，以及一些需要即时推理或领域适应的任务，例如使用一个新单词在一个单词中加扰。句子或执行3位数算术。同时，我们还确定了一些数据集，其中GPT-3的几次学习仍然困难重重，以及一些GPT-3面临与大型Web语料库训练有关的方法论问题的数据集。最后，我们发现GPT-3可以生成新闻文章的样本，人类评估人员很难将其与人类撰写的文章区分开。我们将讨论这一发现以及GPT-3对社会的更广泛影响

近年来，NLP系统已成为一种预训练语言表示的趋势，并以越来越灵活且与任务无关的方式应用于下游传输。首先，使用单词向量[MCCD13，PSM14]学习单层表示并将其馈送到特定于任务的体系结构，然后使用具有多层表示和上下文状态的RNN形成更强的表示[DL15，MBXS17，PNZtY18]（尽管仍然应用于特定任务的体系结构），并且最近已对预训练的递归或转换语言模型[VSP + 17]进行了直接微调，从而完全消除了对特定任务的体系结构[RNSS18，DCLT18，HR18]的需求。最后一个范例在许多具有挑战性的NLP任务（例如阅读理解，问题解答，文本蕴含等）上取得了实质性进展，并且在新的体系结构和算法[RSR + 19，LOG + 19，YDY + 19，LCG + 19]。但是，此方法的主要局限性在于，尽管体系结构与任务无关，但仍需要特定于任务的数据集和特定于任务的微调：要在所需任务上实现强大的性能通常需要对特定任务进行微调。特定于该任务的成千上万个示例的数据集。由于多种原因，希望消除此限制。首先，从实际角度出发，每项新任务都需要有大量带标签示例的数据集，这限制了语言模型的适用性。存在许多可能的有用的语言任务，包括从纠正语法，生成抽象概念的示例到撰写短篇小说的任何内容。对于这些任务中的许多任务，很难收集大型的有监督的训练数据集，尤其是当必须为每个新任务重复该过程时。其次，随着模型的表现力和训练分布范围的缩小，利用训练数据中的虚假相关性的潜力从根本上增长。这会给预训练加微调范式带来问题，在这种情况下，模型被设计得很大，可以在预训练期间吸收信息，但随后会在非常狭窄的任务分布上进行微调。例如，[HLW + 20]观察到，较大的模型并不一定能概括出更好的分布外分布。有证据表明，在该范式下实现的泛化效果可能很差，因为该模型过于针对训练分布，并且无法很好地泛化[YdC + 19，MPL19]。因此，即使在名义上处于名义水平，微调模型在特定基准上的性能也会夸大底层任务的实际性能[GSL + 18，NK19]。第三，人类不需要大型的监督数据集即可学习大多数语言任务-简短的自然语言指令（例如“请告诉我这句话是描述快乐还是悲伤”）或最多仅进行少量演示（例如“此处是两个举止勇敢的人的例子；请举第三个勇敢的例子”）

语言模型元学习。 在无人监督的预训练期间，语言模型会发展出广泛的技能和模式识别能力。 然后，它在推断时使用这些功能来快速适应或识别所需的任务。 我们使用术语“上下文学习”来描述此过程的内部循环，该循环发生在每个序列的前向传递内。 该图中的序列并非旨在表示模型在预训练期间将看到的数据，而是旨在表明有时在单个序列中嵌入了重复的子任务

足以使人们至少能够以合理的能力执行一项新任务。除了指出我们当前NLP技术的概念局限性之外，这种适应性还具有实际优势–它使人类可以无缝地混合在一起或在许多任务和技能之间切换，例如在漫长的对话中进行添加。为了广泛使用，我们希望有一天我们的NLP系统具有相同的流动性和通用性。解决这些问题的一条潜在途径是元学习1 –在语言模型的上下文中，这意味着该模型在训练时会开发出广泛的技能和模式识别能力，然后在推理时使用这些能力快速适应或识别所需的任务（如图1.1所示）。最近的工作[RWC + 19]尝试通过我们所谓的“上下文学习”来完成此任务，使用预先训练的语言模型的文本输入作为任务说明的形式：该模型以自然语言指令和/或条件为条件对该任务进行了一些演示，然后仅通过预测接下来会发生什么，就可以完成该任务的更多实例。尽管已显示出一些初步的希望，但这种方法仍然无法获得比微调更差的结果–例如，[RWC + 19]在自然问题上仅获得4％的收益，甚至其55 F1 CoQa的结果也比其落后35分以上。最先进的。元学习显然需要实质性的改进，以便能够作为解决语言任务的实用方法。语言建模的另一种最新趋势可能提供了前进的道路。近年来，转换器语言模型的容量已从1亿个参数[RNSS18]增至3亿个参数[DCLT18]，增至15亿个参数[RWC + 19]，增至80亿个参数[SPP + 19]，11十亿个参数[RSR + 19]，最后是170亿个参数[Tur20]。每次增加都带来了文本合成和/或下游NLP任务的改善，并且有证据表明，与许多下游任务密切相关的对数丢失遵循随规模而平滑改善的趋势[KMH + 20]。由于上下文学习涉及在模型的参数内吸收许多技能和任务，因此上下文学习能力可能随着规模的增长而显示出类似的强劲增长

在本文中，我们通过训练1750亿个参数自回归语言模型（称为GPT-3）并测量其在上下文中的学习能力来检验该假设。具体来说，我们评估了超过两打NLP数据集的GPT-3，以及旨在测试快速适应不太可能直接包含在训练集中的任务的几个新颖任务。对于每项任务，我们在3种情况下评估GPT-3：（a）“少量学习”或上下文学习，其中我们允许尽可能多的演示适合模型的上下文窗口（通常为10到100），（ b）“一次性学习”，其中我们只允许一个演示；以及（c）“零射击”，其中，不允许演示，并且仅向模型提供自然语言的说明。 GPT-3原则上也可以在传统的微调环境中进行评估，但我们将其留待以后的工作。图1.2说明了我们研究的条件，并显示了对简单任务的少量学习，该简单任务要求模型从单词中删除多余的符号。通过添加自然语言任务描述以及模型上下文中的示例数量，K可以提高模型性能。很少学习会随着模型大小的提高而大大提高。尽管在这种情况下的结果特别引人注目，但是对于我们研究的大多数任务，模型大小和上下文中示例数量的总体趋势仍然适用。我们强调，这些“学习”曲线不涉及梯度更新或微调，而只是增加了作为条件的演示次数。广义上讲，在NLP任务上，GPT-3在零镜头和单镜头设置中取得了可喜的结果，而在少数镜头设置中，有时甚至可以与最先进的设备竞争（甚至超越）。 -由微调的模型掌握）。例如，GPT-3在零连拍设置下在CoQA上达到81.5 F1，在单连拍设置下在CoQA上达到84.0 F1，在少连拍设置下达到85.0 F1。同样，GPT-3在零镜头设置下的TriviaQA上达到64.3％的准确度，在单镜头设置下达到68.0％，在单镜头设置下达到71.2％，最后一个是相对的最新技术调整以相同的账本设置运行的模型。 GPT-3在旨在测试快速适应性或即时推理的任务上也能显示出一枪多语的能力，其中包括解密单词，执行算术运算以及在仅定义一次单词后在句子中使用新颖单词。我们还显示，在很少的情况下，GPT-3可以生成人工评估者难以与人工生成的文章区分开的综合新闻文章。同时，我们还发现一些任务，即使在GPT-3的规模上，也很少有性能问题。这包括自然语言推理任务，例如ANLI数据集，以及一些阅读理解数据集，例如RACE或QuAC。通过展示GPT-3的优点和缺点的广泛特征，包括这些局限性，我们希望能激发对语言模型的少量学习的研究，并提请注意最需要进步的地方。可以从图1.3中看出启发性的整体结果，它汇总了各种任务（尽管不应将其视为严格或有意义的基准）

我们还对“数据污染”进行了系统的研究，这是在诸如Common Crawl之类的数据集上训练高容量模型时出现的一个日益严重的问题，该数据可能潜在地包含来自测试数据集的内容，因为这些内容通常存在于网络上。在本文中，我们开发了系统的工具来测量数据污染并量化其失真影响。尽管我们发现数据污染对大多数数据集的GPT-3性能影响不大，但我们确实确定了一些可能夸大结果的数据集，或者不报告这些数据集的结果，或者用星号标记它们，取决于严重程度。除上述所有功能外，我们还训练了一系列较小的模型（范围从1.25亿个参数更改为130亿个参数），以便将其在零次，一次和几次曝光设置下的性能与GPT-3进行比较。概括地说，对于大多数任务，我们发现在所有三种设置下，模型容量都相对平滑了。一个值得注意的模式是，零，一和少发性能之间的差距通常随模型容量而增大，这可能表明较大的模型是更熟练的元学习者。最后，鉴于GPT-3展示的广泛功能，我们讨论了有关偏见，公平和更广泛的社会影响的担忧，并尝试就此方面对GPT-3的特征进行初步分析。本文的其余部分安排如下。在第2节中，我们介绍了训练和评估GPT-3的方法和方法。第3节介绍了零镜头，单镜头和几镜头设置下所有任务的结果。第4节解决了数据污染（火车测试重叠）的问题。第5节讨论了GPT-3的局限性。第6节讨论了更广泛的影响。第7节回顾了相关工作，第8节总结

我们的基本预训练方法（包括模型，数据和训练）与[RWC + 19]中描述的过程相似，模型大小，数据集大小和多样性以及训练时间的扩展相对简单。我们在上下文学习中的使用也与[RWC + 19]类似，但是在这项工作中，我们系统地探索了在上下文中进行学习的不同设置。因此，我们从明确定义和对比将要评估GPT-3或原则上可以评估GPT-3的不同设置开始本节。这些设置可以看作是它们倾向于依赖多少特定于任务的数据的频谱。具体来说，我们至少可以识别该频谱上的四个点（请参见图2.1）：•精调（FT）是近年来最常用的方法，涉及通过以下方式更新预训练模型的权重：在特定于所需任务的监督数据集上进行培训。通常使用成千上万的标记示例。微调的主要优点是在许多基准上均具有出色的性能。主要缺点是，每个任务都需要一个新的大型数据集，泛化能力很差，分布不全[MPL19]，以及利用训练数据的虚假特征[GSL + 18，NK19]的潜在潜力与人类表现的不公平比较在这项工作中，我们不会微调GPT-3，因为我们的重点是与任务无关的性能，但是原则上可以微调GPT-3，这是未来工作的有希望的方向。 •少拍（FS）是我们在这项工作中使用的术语，是指在推理时将模型作为条件[RWC + 19]进行一些演示的条件，但不允许权重更新。如图2.1所示，对于一个典型的数据集，一个示例具有上下文和所需的补全（例如英语句子和法语翻译），并通过给出K个上下文和补全示例的少量镜头，然后是最后一个示例与上下文相关，期望模型能够提供完成。我们通常将K设置在10到100的范围内，因为这是模型的上下文窗口中可以容纳的示例数量（nctx = 2048）。少拍的主要优点是，大大减少了对特定任务数据的需求，并减少了从庞大但狭窄的微调数据集中学习过窄分布的可能性。主要缺点是，到目前为止，这种方法的结果要比最新的微调模型差很多。同样，仍然需要少量的任务特定数据。顾名思义，此处针对语言模型描述的快速学习与ML [HYC01，VBL + 16]其他上下文中使用的快速学习相关-都涉及基于广泛任务分配的学习（在这种情况会隐含在预训练数据中），然后迅速适应新任务。 •单拍（1S）与少拍相同，除了除了对任务的自然语言描述外，仅允许一个演示，如图1所示。区分单拍和少拍的原因零射（下图）是它与某些任务传达给人类的方式最接近。例如，当要求人员生成有关人员服务（例如Mechanical Turk）的数据集时，通常会演示该任务。相反，如果没有给出示例，有时很难传达任务的内容或格式

零射击（0S）与单发射击相同，只是不允许进行演示，并且仅向模型提供描述任务的自然语言指令。这种方法提供了最大的便利性，鲁棒性的潜力以及避免了虚假相关性（除非它们在大量的预训练数据中非常广泛地发生），但这也是最具挑战性的设置。在某些情况下，如果没有先前的示例，对于人类来说甚至可能很难理解任务的格式，因此在某些情况下，此设置“非常不容易”。例如，如果有人要求“制作200m破折号的世界记录表”，则此请求可能会模棱两可，因为可能不清楚表的确切格式或应包含的格式（甚至要小心澄清，准确理解所需内容可能会很困难）。但是，至少在某些设置下，零镜头最接近人类执行任务的方式–例如，在图2.1的翻译示例中，人类可能仅从文本指令中知道要做什么

我们使用与GPT-2 [RWC + 19]相同的模型和体系结构，包括其中描述的修改后的初始化，预规范化和可逆记号化，不同之处在于，在以下各层中使用了交替的密集和局部条带的稀疏注意模式变压器，类似于稀疏变压器[CGRS19]。为了研究机器学习性能对模型大小的依赖性，我们训练了8种不同大小的模型，范围从1.25亿个参数到1,750亿个参数，超过三个数量级，最后一个模型称为GPT-3。先前的工作[KMH + 20]建议，在有足够的训练数据的情况下，验证损失的缩放比例应近似为随大小变化的平稳幂定律。许多不同规模的训练模型使我们能够针对验证损失和下游语言任务测试该假设。表2.1列出了我们8种模型的大小和体系结构。这里nparamsis可训练参数的总数，nlayersis总层数，dmodel是每个瓶颈层的单位数（我们总是前馈层是瓶颈层大小的四倍，dff = 4 * dmodel）和dheadis每个注意头的尺寸。所有模型都使用nctx = 2048个令牌的上下文窗口。我们沿着深度和宽度维度跨GPU划分模型，以最大程度地减少节点之间的数据传输。根据计算效率和跨GPU的模型布局中的负载平衡来选择每个模型的精确架构参数。先前的工作[KMH + 20]表明，验证损失在相当宽的范围内对这些参数并不很敏感

语言模型的数据集已迅速扩展，最终达到了构成近一万亿个单词的Common Crawl数据集2 [RSR + 19]。如此庞大的数据集足以训练我们最大的模型，而无需在同一序列上进行两次更新。但是，我们发现，与经过精心挑选的数据集相比，未经过滤或经过轻微过滤的Common Crawl版本的质量往往较低。因此，我们采取了3个步骤来提高数据集的平均质量：（1）基于与一系列高质量参考语料库的相似性，下载并过滤了CommonCrawl版本；（2）我们在文档级执行了重复数据删除数据集内和数据集之间，以防止冗余并保留我们保留的验证集的完整性，以作为过拟合的准确度量，并且（3）我们还将已知的高质量参考语料库添加到训练组合中以增强CommonCrawl并增加其多样性。前两点的详细信息（Common Crawl的处理）在附录A中进行了描述。对于第三点，我们添加了一些精选的高质量数据集，包括WebText数据集的扩展版本[RWC + 19]，这些数据集是通过在以下位置抓取链接来收集的较长的时间，首先在[KMH + 20]中介绍了两种基于互联网的图书资料集（Books1和Books2）和英语Wikipedia。表2.2显示了我们在训练中使用的最终数据集。 CommonCrawl数据是从2016年至2019年的每月41个CommonCrawl分片中下载的，构成了过滤前的45TB压缩明文和过滤后的570GB，大致相当于4000亿字节对编码的令牌。请注意，在训练过程中，并不是按大小对数据集进行采样，而是我们认为更高质量的数据集采样频率更高，因此，在训练过程中CommonCrawl和Books2数据集采样的次数少于一次，而其他数据集则采样2 -3次。这本质上是接受少量的过度拟合以换取更高质量的训练数据

对于在大量互联网数据上进行预训练的语言模型，尤其是具有记忆大量内容的能力的大型模型，主要的方法论关注是在预训练过程中无意中看到了它们的测试或开发集，从而可能污染下游任务。 为了减少这种污染，我们寻求并尝试消除与本文研究的所有基准测试的开发和测试集的任何重叠。 不幸的是，过滤中的错误导致我们忽略了一些重叠，并且由于训练的成本，重新训练模型是不可行的。 在第4节中，我们描述了剩余重叠的影响，在以后的工作中，我们将更积极地消除数据污染

