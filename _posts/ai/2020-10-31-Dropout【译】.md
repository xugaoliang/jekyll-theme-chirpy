---
title: "Dropout【译】"
categories:
    - AI
tags:
  - 深度学习
---

论文：Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf). Journal of Machine Learning Research 15 (2014) 1929-1958

## 摘要

具有大量参数的深度神经网络是非常强大的机器学习系统。但是，过度拟合是此类网络中的一个严重问题。大型网络的使用速度也很慢，因此难以通过在测试时结合许多不同大型神经网络的预测来处理过度拟合问题。辍学是一种解决此问题的技术。关键思想是在训练过程中从神经网络中随机删除单元（及其连接）。这样可以防止单元之间的过度配合。在训练期间，从指数数量的不同“瘦”网络中抽取样本。在测试时，仅通过使用权重较小的单个未精简网络，就可以很容易地近似平均所有这些精简网络的预测结果。与其他正则化方法相比，这显着减少了过度拟合并带来了重大改进。我们表明，辍学可以提高视觉，语音识别，文档分类和计算生物学等有监督学习任务上神经网络的性能，并在许多基准数据集上获得最新的结果。关键字：神经网络，正则化，模型组合，深度学习

## 1. 摘要

深度神经网络包含多个非线性隐藏层，这使它们具有很高的表达能力，可以学习其输入和输出之间非常复杂的关系。 但是，在训练数据有限的情况下，许多复杂的关系将是采样噪声的结果，因此，即使它们来自相同的分布，它们也将存在于训练集中，但不存在于实际测试数据中。 这导致过度拟合，并且已经开发出许多方法来减小它。 这些措施包括一旦验证集的性能开始变差就停止训练，引入各种重量惩罚，例如L1和L2正则化以及轻柔的重量分配（Nowlan和Hinton，1992）。 借助无限制的计算，“规范化”固定大小模型的最佳方法是对所有可能的参数设置的预测取平均值，并通过给定训练数据后验概率。 对于简单模型或小型模型，有时可以很好地近似得出（Xiong等人，2011； Salakhutdinov和Mnih，2008），但是我们希望使用少得多的计算来接近贝叶斯黄金标准的性能。 我们建议通过对共享参数的指数式学习模型的预测的均等加权几何平均值进行近似来实现

模型组合几乎总是可以提高机器学习方法的性能。但是，对于大型神经网络，将许多单独训练的网络的输出平均的显而易见的想法非常昂贵。当各个模型彼此不同时，合并多个模型最有帮助，并且为了使神经网络模型不同，它们应该具有不同的体系结构或接受不同的数据训练。训练许多不同的架构非常困难，因为为每种架构找到最佳的超参数是一项艰巨的任务，而训练每个大型网络则需要大量的计算。而且，大型网络通常需要大量的训练数据，并且可能没有足够的数据来在数据的不同子集上训练不同的网络。即使一个人能够训练许多不同的大型网络，在需要快速响应的应用程序中也不可能在测试时全部使用它们。辍学是解决这两个问题的技术。它可以防止过度拟合，并提供一种有效地近似指数化组合许多不同神经网络体系结构的方法。术语“退出”是指在神经网络中退出单位（隐藏和可见）。通过丢弃一个单元，我们的意思是暂时将其及其所有传入和传出连接从网络中删除，如图1所示。要丢弃的单元的选择是随机的。在最简单的情况下，每个单元都以固定的概率p保持独立于其他单元的概率，其中p可以使用验证集进行选择，也可以简单地设置为0.5，这似乎对于广泛的网络和最佳网络都是最佳的。任务。但是，对于输入单元，保留的最佳概率通常接近于1而不是接近0.5

将缺失应用于神经网络等于从中采样“精简”网络。精简的网络由所有幸存的单元组成（图1b）。具有n个单位的神经网络可以看作是2个不可能的细化神经网络的集合。这些网络均共享权重，因此参数总数仍为O（n2）或更小。对于每个培训案例的每次演示，都会对新的精简网络进行采样和培训。因此，训练具有失落的神经网络可以看作是训练具有广泛权重的2n个瘦化网络的集合，其中每个瘦化网络很少（如果有的话）受到训练。在测试时，要显式平均来自指数稀疏模型的预测是不可行的。但是，非常简单的近似平均方法在实践中效果很好。想法是在测试时使用单个神经网络而不会出现辍学现象。该网络的权重是训练后的权重的按比例缩小版本。如果在训练期间以概率p保留一个单位，则在测试时将该单位的输出权重乘以p，如图2所示。这确保了对于任何隐藏的单位，预期输出（在用于训练时丢弃单位的分布下）时间）与测试时间的实际输出相同。通过进行这种缩放，可以将具有共享权重的2n个网络合并为一个神经网络，以在测试时使用。我们发现，与使用其他正则化方法进行训练相比，训练具有辍学的网络并在测试时使用此近似平均方法可导致针对各种分类问题的泛化误差大大降低。辍学的想法不仅限于前馈神经网络。它可以更普遍地应用于图形模型，例如Boltzmann Machines。在本文中，我们介绍了辍学受限玻尔兹曼机模型，并将其与标准受限玻尔兹曼机（RBM）进行了比较。我们的实验表明，在某些方面，辍学的RBM优于标准RBM。此篇文章的结构如下。第2节介绍了此想法的动机。第3节介绍了相关的先前工作。第4节正式描述了辍学模型。第5节给出了用于训练辍学网络的算法。在第6节中，我们介绍了我们的实验结果，其中我们将辍学应用于不同领域的问题，并将其与其他形式的正则化和模型组合进行了比较。第7节分析了辍学对神经网络不同属性的影响，并描述了辍学如何与神经网络的超参数相互作用。第8节介绍了Dropout RBM模型。在第9节中，我们探讨了使辍学边缘化的想法。在附录A中，我们提供了实用指南训练辍学网。 这包括对训练辍学网络时选择超参数的实际考虑因素的详细分析

## 2. 动机

辍学的动机来自性别在进化中的作用的理论（Livnat等，2010）。有性生殖包括获取一个亲本的一半基因和另一个亲本的一半基因，添加非常少量的随机突变，然后将它们组合以产生后代。无性恋的替代方法是创造一个后代，其后代的父母基因稍有突变。无性繁殖应该是优化个体适应性的更好方法，因为可以将已经很好地协同工作的一组良好的基因直接传递给后代。另一方面，有性生殖很可能破坏这些共同适应的基因组，尤其是如果这些基因组很大，并且从直觉上讲，这将降低已经进化出复杂的共同适应性的生物体的适应性。但是，有性生殖是大多数高级生物进化的方式。有性繁殖优势的一种可能解释是，从长远来看，自然选择的标准可能不是个体适应性而是基因的混合能力。一组基因能够与另一组随机基因很好地协同工作的能力使它们更加强大。由于一个基因不能一直依赖大量伙伴，因此它必须学会独自或与少数其他基因协作做一些有用的事情。根据这一理论，有性生殖的作用不仅是允许有用的新基因在整个人群中传播，而且还通过减少复杂的共适应来促进这一过程，这将减少新基因改善人类适应性的机会。个人。类似地，经过辍学训练的神经网络中的每个隐藏单元都必须学会与其他单元的随机选择样本一起工作。这应该使每个隐藏单元都更强大，并促使其自行创建有用的功能，而不必依靠其他隐藏单元来纠正其错误。但是，层中的隐藏单元仍将学习彼此做不同的事情。可能会想到，通过为每个隐藏单元制作许多副本，网络将变得更强大，从而可以防止丢失，但是，由于复制代码是处理噪声通道的一种糟糕方法，原因完全相同，因此这是一个较差的解决方案。想到成功的阴谋是一个密切相关但略有不同的辍学动机。十个合谋涉及五个人，这可能是比一个大合谋需要五十个人正确发挥自己的作用的更好的破坏方式。如果情况没有改变，并且有足够的时间进行彩排，那么一个大的阴谋可以很好地发挥作用，但是在非平稳的情况下，阴谋越小，它仍然可以继续工作的机会越大。可以训练复杂的联合适应在训练集上很好地工作，但是在新颖的测试数据上，它们比实现同一件事的多个简单的联合适应更容易失败

## 3. 相关工作

辍学可以解释为一种通过向其隐藏单元中添加噪声来规范化神经网络的方法。 Vincent等人先前已在去噪自动编码器（DAE）的上下文中使用了将噪声添加到单元状态的想法。 （2008年，2010年）将其添加到自动编码器的输入单元，并训练网络以重建无噪声输入。我们的工作通过显示辍学也可以有效地应用于隐藏层，并且可以将其解释为模型平均的一种形式，扩展了这一思想。我们还表明，添加噪声不仅对无监督的特征学习有用，而且可以扩展到有监督的学习问题。实际上，我们的方法可以应用于其他基于神经元的体系结构，例如Boltzmann Machines。虽然5％噪声通常最适合DAE，但我们发现在测试时采用的权重缩放程序使我们可以使用更高的噪声水平。经常发现删除20％的输入单元和50％的隐藏单元是最佳的。由于丢包可以看作是一种随机的正则化技术，因此自然而然地考虑通过边缘化噪声获得的确定性副本。在本文中，我们表明，在简单情况下，辍学可以被分析性地边缘化以获得确定性正则化方法。最近，van der Maaten等人。 （2013年）还探讨了与不同的指数族噪声分布相对应的确定性正则化器，包括辍学（他们称之为“空白噪声”）。但是，它们将噪声应用于输入，并且仅探索没有隐藏层的模型。 Wang和Manning（2013）提出了一种通过边缘化降落噪声来加快掉落速度的方法。 Chen等。 （2012年）探讨了在去噪自动编码器的背景下的边缘化。在压差中，我们在噪声分布下随机地最小化损失函数。这可以看作是使预期损失函数最小化。 Globerson和Roweis的先前工作（2006年）； Dekel等。 （2010）探索了一种替代设置，当对手选择要丢弃的单位时，损失最小。在这里，代替噪声分布，可以丢弃的最大单元数是固定的。但是，此工作也不会探索具有隐藏单元的模型

## 4. 模型描述

本节介绍了辍学神经网络模型。 考虑具有L个隐藏层的神经网络。 令l∈{1，。 。 。 ，L}索引网络的隐藏层。 令z（l）表示输入到第l层的向量，y（l）表示从第l层的输出向量（y（0）= x是输入）。 W（l）和b（l）是第l层的权重和偏差。 标准神经网络（图3a）的前馈操作可以描述为（对于l∈{0，...，L-1}和任何隐藏的单位i

其中f是任何激活函数，例如f（x）= 1 /（1 + exp（-x））。 退出后，前馈操作变为（图3b

∗表示元素乘积。 对于任何l层，r（l）是一个独立的伯努利随机变量的向量，每个随机变量的概率p为1。对该向量进行采样并将其与该层的输出y（l）逐个元素相乘，以创建 稀疏的输出？ y（l）。 稀疏的输出然后用作下一层的输入。 此过程应用于每一层。 这相当于从较大的网络中采样子网。 为了学习，损失函数的导数通过子网反向传播。 在测试时，权重按W（l test）= pW（l）进行缩放，如图2所示。使用所得的神经网络而不会出现丢失

## 5. 学习辍学网

本节介绍了训练辍学神经网络的过程

5.1反向传播可以使用随机梯度下降以类似于标准神经网络的方式训练辍学神经网络。唯一的区别是，对于小批量生产中的每个培训案例，我们通过剔除单元来对精简网络进行采样。该训练案例的正向和反向传播仅在此精简网络上进行。每个参数的梯度是在每个微型批次的训练案例中平均的。任何不使用参数的训练案例都会为该参数贡献零梯度。已经使用了许多方法来改善随机梯度下降，例如动量，退火的学习速率和L2重量衰减。还发现这些方法对于辍学神经网络也很有用。发现一种特殊形式的正则化对于丢包特别有用-将每个隐藏单元的传入权重向量的范数约束为固定常数c的上限。换句话说，如果w表示入射到任何隐藏单元上的权重矢量，则在约束条件|| w ||2≤c的情况下对神经网络进行优化。每当w离开时，就在优化过程中通过将w投射到半径为c的球的表面上来施加此约束。这也称为max-norm正则化，因为它意味着任何权重的范数可以采用的最大值是c。常数c是可调超参数，可使用验证集确定。 Max-norm正则化以前已在协作过滤的背景下使用（Srebro和Shraibman，2005）。即使不使用辍学方法，通常也可以提高深度神经网络的随机梯度下降训练的性能。尽管仅辍学会带来显着的改善，但使用辍学以及最大范数正则化，大的衰减学习率和高动量比仅使用辍学有明显的提升。可能的论据是，将权重向量约束在固定半径的球内，可以使用巨大的学习率而不会使权重爆炸。然后，由辍学人员提供的噪音使优化过程可以探究重量空间的不同区域，否则这些区域将难以到达。随着学习率的下降，优化过程将花费更短的时间，从而减少了探索，最终达到了最小化

5.2无监督的预训练神经网络可以使用RBM（Hinton和Salakhutdinov，2006），自动编码器（Vincent等，2010）或Deep Boltzmann Machines（Salakhutdinov和Hinton，2009）进行预训练。预培训是利用未标记数据的有效方法。在某些情况下，预训练后再进行反向传播微调已显示出比随机初始化进行微调显着提高的性能。可以将压差应用于使用这些技术进行了预训练的微调网络。预训练程序保持不变。预训练获得的权重应按1 / p放大。这样可以确保对于每个单元，在随机辍学情况下，其预期输出将与预训练期间的输出相同。最初，我们担心辍学的随机性可能会抹去预训练体重中的信息。当微调期间使用的学习速率与随机初始化网络的最佳学习速率相当时，就会发生这种情况。如何 - 以往，当学习率被选为较小，在预训练的权重的信息似乎被保留，我们能够得到改进，最终泛化误差的条款相比，不使用时辍学细化和微调

6.实验结果我们训练了辍学神经网络来解决不同领域数据集的分类问题。 我们发现，与未使用辍学的神经网络相比，辍学改善了所有数据集的泛化性能。 表1简要说明了这些数据集。 数据集为•MNIST：手写数字的标准玩具数据集。 •TIMIT：用于干净语音识别的标准语音基准。 •CIFAR-10和CIFAR-100：微小的自然图像（Krizhevsky，2009年）。 •街景门牌号码数据集（SVHN）：由Google街景视图收集的门牌号码图片（Netzer等，2011）。 •ImageNet：大量自然图像。 •Reuters-RCV1：路透社新闻集文章
备选剪接数据集：用于预测备选基因剪接的RNA特征（Xiong等，2011


我们选择了一组不同的数据集，以证明辍学是改善神经网络的通用技术，并不特定于任何特定的应用领域。 在本节中，我们提供一些关键的结果，这些结果表明了辍学的有效性。 附录B中提供了所有实验和数据集的更详细说明

6.1图像数据集上的结果我们使用了五个图像数据集来评估辍学-MNIST，SVHN，CIFAR-10，CIFAR-100和ImageNet。 这些数据集包括不同的图像类型和训练集大小。 在所有这些数据集上都能获得最新结果的模型使用了辍学

MNIST数据集由28×28像素的手写数字图像组成。 任务是将图像分类为10位数字。 表2比较了辍学与其他技术的性能。 置换不变性的最佳性能神经网络

